[
  {
    "path": "posts/2022-02-28-pandaswishes/",
    "title": "pandas Wishes",
    "description": "Features that I miss in pandas coming from the tidyverse.",
    "author": [
      {
        "name": "Bruno Testaguzza Carlin",
        "url": "https://twosidesdata.netlify.app/"
      }
    ],
    "date": "2022-02-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nSetup\r\nLink to the GitHub source\r\ncode\r\nLibraries in R\r\nLibraries in Python\r\nThe Data: Penguins!\r\nWhat does the dataset\r\nlook like?\r\nHow do they look like?\r\nInteraction among\r\nvariables and species\r\n\r\n\r\nMy experience\r\nThe tasks of pandas\r\nframework?\r\nThe current landscape\r\nBase R\r\nData Prep\r\nSlicing\r\nComplex manipulation\r\n\r\ndata.table\r\nData Prep\r\nComplex manipulation\r\n\r\nDplyr / the tidyverse\r\nSimple Manipulation\r\nComplex Manipulation\r\n\r\npandas\r\nSet up data\r\nSlicing\r\nSimple Manipulation\r\nComplex Manipulations\r\n\r\nPySpark/Koalas\r\n\r\nGrivances with pandas\r\nWhat pandas has and it\r\nshouldn’t\r\nWhat pandas hasn’t and it\r\nshould\r\nLast updated on\r\nReferences\r\n\r\n\r\nSetup\r\nLink to the GitHub source\r\ncode\r\n\r\n\r\n See the source rmd file\r\n\r\n\r\nLibraries in R\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(data.table)\r\n\r\n\r\n\r\nLibraries in Python\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nThe Data: Penguins!\r\nHorst AM,\r\nHill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago\r\n(Antarctica) penguin data. R package version 0.1.0.\r\nhttps://allisonhorst.github.io/palmerpenguins/\r\nI choose the Palmer Penguins Dataset. Some call it the new iris after\r\nthe iris dataset was rightly criticized for Eugenics issues. You can\r\nread more about it here Iris dataset\r\nretirement.\r\nWhat does the dataset look\r\nlike?\r\n\r\n\r\npenguins <- palmerpenguins::penguins\r\n\r\nhead(penguins)\r\n\r\n\r\n# A tibble: 6 x 8\r\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\r\n  <fct>   <fct>              <dbl>         <dbl>             <int>\r\n1 Adelie  Torgersen           39.1          18.7               181\r\n2 Adelie  Torgersen           39.5          17.4               186\r\n3 Adelie  Torgersen           40.3          18                 195\r\n4 Adelie  Torgersen           NA            NA                  NA\r\n5 Adelie  Torgersen           36.7          19.3               193\r\n6 Adelie  Torgersen           39.3          20.6               190\r\n# ... with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\r\n\r\nglimpse(penguins,width = 50)\r\n\r\n\r\nRows: 344\r\nColumns: 8\r\n$ species           <fct> Adelie, Adelie, Adelie~\r\n$ island            <fct> Torgersen, Torgersen, ~\r\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, ~\r\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, ~\r\n$ flipper_length_mm <int> 181, 186, 195, NA, 193~\r\n$ body_mass_g       <int> 3750, 3800, 3250, NA, ~\r\n$ sex               <fct> male, female, female, ~\r\n$ year              <int> 2007, 2007, 2007, 2007~\r\n\r\nThe dataset consists of 344 samples of three species of Penguins.\r\nHow do they look like?\r\nThe three species have different characteristics.\r\nArtwork by @allison_horstAnd what the hell is Bill Lenght and Depth?\r\nArtwork by @allison_horstInteraction among\r\nvariables and species\r\nCredit\r\nto Allison once again\r\nPenguin mass vs. flipper\r\nlength\r\n\r\n\r\n\r\nAs expected from the picture, the Gento species is a bit bigger, and\r\nwe can see a heavy correlation between Flipper length and weight.\r\nBill length vs. depth\r\n\r\n\r\n\r\nAs we saw in the picture, the Adelie has smaller-length Bills.\r\nThe Gentoo has much more elongated ones than the Adelie, but they look\r\n‘flatter,’ or as the dataset calls it, they have a smaller Bill depth.\r\nThe Chinstrap are positioned on the bigger side of both metrics.\r\nImpact of sex on size\r\n\r\n\r\n\r\nMy experience\r\nAt this point I have worked more than 4 years with both Python and R,\r\nPython has been my language choice when interacting with colleges simply\r\nbecause of culture or better support in platforms such as AWS, but it is\r\nimportant to understand that R was my first data language (I dabbled in\r\nsome Macros on excel before) and I have used it since graduation more\r\nthan 7 years ago.\r\nThe tasks of pandas\r\nframework?\r\nFirstly, I call it a framework because pandas comes bundled with\r\nNumPy and other friends like matplot or seaborn. It is fair to compare\r\nthem as a bundle since the competition is also split among many\r\npackages. I will borrow a picture from the tidyverse to explain where\r\npandas stands on the analysis workflow.\r\nCredit\r\ntohttps://oliviergimenez.github.io/pandas Is responsible for the Import -> Tidy -> Transform\r\npart\r\nBut it can sometimes reach a little bit into Visualize by delegating\r\nwhen necessary. If your communication task is a graph, you can use\r\nmatplot or seaborn, and there are also some simple APIs inside pandas\r\nthat can plot basic graphs.\r\nIf you are importing from parquet, sometimes pandas calls on the\r\narrow package.\r\nThere are many other examples, but I think of pandas as this central\r\nplayer on the data workflow of the Python ecosystem, it is unmatched in\r\nits amount of methods, and therefore it should be expected that some\r\nthings could use some improvement, and that is the point of this post, I\r\nwant pandas to change for the better, I want to discuss why pandas is in\r\nthis state of right now and what I would like to see instead.\r\nThe current landscape\r\nIn R, I have used three different frameworks when dealing with data\r\nthat can be considered a problem. Still, there is a crucial difference\r\nwhen comparing Python and R, you will usually find competition inside R,\r\nand it happens on every sphere, modeling has the tidymodels vs. mlr\r\nfight, visualization has plotly vs. ggplot2, statistical tests sometimes\r\nhave tens of different implementations from all over the world, etc. As\r\nan economist, I see this competition with good eyes. The competition has\r\nfostered fast and tested change on multiple instances as competing ##\r\nThe timeline\r\nFirst there was Base R created on August 1993; 28 years ago according\r\nto Wikipedia, but with many more years of baggage from the S era. Then\r\npandas starts development around 2005 and gets open sourced on 2008, it\r\nis a game changer and vastly more declarative and faster than Base R.\r\ndata.table comes along on the premise of speed matters a lot, remember\r\nthat it is 2006 and the single big machine paradigm was the end all for\r\nbig data analysis, maybe it wasn’t even called big data back then. the\r\ntidyverse releases dplyr on the back of the huge success of ggplot2, it\r\nwasn’t even called the tidyverse back then. And finally Koalas get\r\ncreated on 2019 to help pandas scale with PySpark, it eventually get\r\nincorporated back into PySpark as the pyspark.pandas api.\r\nBase R\r\n\r\nThe oldest, most of the time slowest way, base R has scared so many\r\naway from r. You will see some nasty behaviors here, like the famous\r\npartial string matching or the very crypt function names and arguments.\r\nBase R feels dated because it is dated, now more than 20 years old, this\r\nframework inspired pandas, but you need some patience if you are using\r\nit. There is an excellent book about all of Base R’s little details that\r\nI highly recommend called The R\r\nInferno\r\nBase r implement some design ideas that users of the pandas ecosystem\r\nwill recognize, like the dreadful row names pandas turned into indexes\r\nand using square brackets for 2d manipulation of data.\r\nSome details of R for the Python folks, R doesn’t expect you to know\r\nwhat a pointer is because R doesn’t expect you to be a regular\r\nprogrammer. Base R envisions a statistician with some thousand lines of\r\nbeautiful math that got turned into a package. This means that if you\r\nassign a copy of a data.frame to a new name, R initially creates a\r\npointer, and eventually, if you have changed it in a destructible way, R\r\nautomatically copies it into another new object. There is no need to\r\nkeep manually using the copy method as in pandas.\r\nData Prep\r\n\r\n\r\npenguins_base <- penguins |> as.data.frame()\r\nrow.names(penguins_base) <- str_c('penguin_',row_number(penguins_base$species))\r\n\r\nhead(penguins_base) |> knitr::kable()\r\n\r\n\r\n\r\nspecies\r\nisland\r\nbill_length_mm\r\nbill_depth_mm\r\nflipper_length_mm\r\nbody_mass_g\r\nsex\r\nyear\r\npenguin_1\r\nAdelie\r\nTorgersen\r\n39.1\r\n18.7\r\n181\r\n3750\r\nmale\r\n2007\r\npenguin_2\r\nAdelie\r\nTorgersen\r\n39.5\r\n17.4\r\n186\r\n3800\r\nfemale\r\n2007\r\npenguin_3\r\nAdelie\r\nTorgersen\r\n40.3\r\n18.0\r\n195\r\n3250\r\nfemale\r\n2007\r\npenguin_4\r\nAdelie\r\nTorgersen\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\n2007\r\npenguin_5\r\nAdelie\r\nTorgersen\r\n36.7\r\n19.3\r\n193\r\n3450\r\nfemale\r\n2007\r\npenguin_6\r\nAdelie\r\nTorgersen\r\n39.3\r\n20.6\r\n190\r\n3650\r\nmale\r\n2007\r\n\r\nSlicing\r\nNow let’s use some base R\r\nYou can filter in a 2d manner based on df[x,y] just like pandas\r\n.loc\r\n\r\n\r\npenguins_base['penguin_1',]\r\n\r\n\r\n          species    island bill_length_mm bill_depth_mm\r\npenguin_1  Adelie Torgersen           39.1          18.7\r\n          flipper_length_mm body_mass_g  sex year\r\npenguin_1               181        3750 male 2007\r\n\r\n\r\n\r\npenguins_base[c('penguin_1','penguin_10'),c('species','island')]\r\n\r\n\r\n           species    island\r\npenguin_1   Adelie Torgersen\r\npenguin_10  Adelie Torgersen\r\n\r\nIt even feels like pandas all the way into it randomly deciding to\r\nchange my data types.\r\n\r\n\r\npenguins_base[c('penguin_1','penguin_10'),c('species')]\r\n\r\n\r\n[1] Adelie Adelie\r\nLevels: Adelie Chinstrap Gentoo\r\n\r\nYep, we just fell out of a data.frame straight into a vector of the\r\nfactor class, fantastic.\r\nComplex manipulation\r\nLet’s try to get the mean of the kgs of the females by species.\r\nDoing some complex calculations on base R feels like a chore, but\r\nsome functions that work as an apply on steroids may usually.\r\nYou will also rely heavily on saving intermediary df’s unless you\r\noverwrite the original, or you cheat a little bit and use pipes from the\r\ntidyverse (I don’t even think of it as cheating anymore as pipes come\r\nnatively with R since the 4.0 release), as I will explain the tidyverse\r\nis a superset of base, meaning that it can be used inside of the Base R\r\nworkflow. It can borrow functions from base as well. Some may call this\r\na modern Base R code as it would not run on earlier than 4.0 versions of\r\nR. This also enables shorter anonymous functions using ‘' instead of\r\n’function’.\r\n\r\n\r\nresults_base <- penguins_base[penguins_base['sex'] == 'female',] |>\r\n  with(aggregate(x =body_mass_g,by = list(species),FUN = \\(x) mean(x)/1000))\r\n\r\nresults_base\r\n\r\n\r\n    Group.1        x\r\n1    Adelie 3.368836\r\n2 Chinstrap 3.527206\r\n3    Gentoo 4.679741\r\n\r\nNice looking table, It is hell to use multiple functions, but if you\r\nknow what you are doing is simple, Base R gets the job done with no\r\nimports… if you still care about that.\r\ndata.table\r\nThe motto here is gotta go fast\r\nGoing as far as naturally parallelize execution on local cores when\r\npossible, some love the syntax, I honestly think it is the worst out of\r\nall the options, but when speed on a single machine is relevant\r\n(something that I encounter less and less as we will discuss later)\r\ndata.table really shines, outperforming just about anything I have ever\r\nused on Python and R.\r\nData Prep\r\n\r\n\r\npenguins_data_table <- as.data.table(penguins)\r\n\r\n\r\n\r\nComplex manipulation\r\n\r\n\r\npenguins_data_table[,list(species,body_mass_g)]\r\n\r\n\r\n       species body_mass_g\r\n  1:    Adelie        3750\r\n  2:    Adelie        3800\r\n  3:    Adelie        3250\r\n  4:    Adelie          NA\r\n  5:    Adelie        3450\r\n ---                      \r\n340: Chinstrap        4000\r\n341: Chinstrap        3400\r\n342: Chinstrap        3775\r\n343: Chinstrap        4100\r\n344: Chinstrap        3775\r\n\r\npenguins_data_table[species %in% c('Adelie','Gentoo') & sex == 'female',list(species,body_mass_g)][,lapply(.SD,mean,na.rm = TRUE),species]\r\n\r\n\r\n   species body_mass_g\r\n1:  Adelie    3368.836\r\n2:  Gentoo    4679.741\r\n\r\nIt produces these magical one-liners with speed to spare. The problem\r\nis that I can barely glimpse what I did here, as almost all of the\r\nexecution depends on you remembering this model behind the scenes.\r\nDT[i, j, by]\r\ni = order by | select\r\nj = update\r\nby = group by\r\nAnd trust me when I say it gets complicated data.table is Turing\r\ncomplete as all options here are, and it is out there performing all of\r\nthe functions of either dplyr or pandas, with just three arguments! That\r\nproduces some of the most confusing pieces of code you will ever read,\r\nat least the data.table team killed the idea of row names as well.\r\nDplyr / the tidyverse\r\nStorybench picture of the\r\ntidyverseMy clear favorite, in a perfect world, everyone should know the\r\ntidyverse for the power it brings on expressing ideas about data with\r\nstraightforward declarative syntax. This is very much the empowered\r\nversion of SQL. A nice thing that I already showed on the base R part is\r\nthat the tidyverse is only a part of the R ecosystem, meaning you can\r\nget your old statistic operations and just plug it into place. I will\r\nfurther detail how easy it is to develop an extension for the tidyverse\r\nbut first, let’s see some syntax.\r\nSimple Manipulation\r\n\r\n\r\npenguins |>\r\n  filter(species %in% c('Adelie', 'Gentoo'),\r\n         sex == 'female') |> \r\n  group_by(species) |> \r\n  summarise(body_mass_g_to_kg = mean(body_mass_g)/1000)\r\n\r\n\r\n# A tibble: 2 x 2\r\n  species body_mass_g_to_kg\r\n  <fct>               <dbl>\r\n1 Adelie               3.37\r\n2 Gentoo               4.68\r\n\r\nThis is an example of what dplyr can do while remaining very similar\r\nto English, you can opt into named arguments that are very well thought\r\nout, some of which have gone into twitter polls, the team at RStudio\r\nclearly thinks about usage and is willing to redesign old parts of the\r\nsystems to reach new usability levels.\r\nComplex Manipulation\r\n\r\n\r\nresult_tidyverse <- penguins |>\r\n  select(-year) |>\r\n  filter(species %in% c('Adelie', 'Gentoo'),\r\n         sex == 'female') |>\r\n  group_by(species) |>\r\n  select(where(is.numeric)) |>\r\n  summarise(across(\r\n    .cols = where(\\(x) mean(x) > 188),\r\n    .fns = list(median = median, mean = mean),\r\n    .names = \"{.fn}-{.col}\"\r\n  )) |>\r\n  mutate(across(\r\n    .cols = ends_with('_g'),\r\n    .fns = list(to_kg = \\(x) x / 1000),\r\n    .names = \"{.col}-{.fn}\"\r\n  ))\r\n\r\nresult_tidyverse |> rmarkdown::paged_table()\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"species\"],\"name\":[1],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"median-flipper_length_mm\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean-flipper_length_mm\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median-body_mass_g\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean-body_mass_g\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"median-body_mass_g-to_kg\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"mean-body_mass_g-to_kg\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Adelie\",\"2\":\"188\",\"3\":\"187.7945\",\"4\":\"3400\",\"5\":\"3368.836\",\"6\":\"3.4\",\"7\":\"3.368836\"},{\"1\":\"Gentoo\",\"2\":\"212\",\"3\":\"212.7069\",\"4\":\"4700\",\"5\":\"4679.741\",\"6\":\"4.7\",\"7\":\"4.679741\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nThis is incredibly similar to my usage of data manipulation in the\r\nreal world. Some functions are simple, like some business metric is\r\nbetter on a kg basis instead of g, while others empower you to write\r\nincredibly succinct syntax that feels like a superpower for your\r\nlaziness. You start to write smaller and smaller code to deal with more\r\nand more complex problems. I realize that most of what is in here exists\r\nsolely on the tidyverse (for now) and that newcomers may not understand\r\nsomewhat complex functions like across the first time they try to use\r\nit. Still, it is such a game-changer that functions like across exist,\r\nthe alternatives being you sometimes writing tens of column names, or\r\nthat you pre-compute a list as I will show it in pandas.\r\nAlso, you can see how the tidyverse easily interacts with custom\r\nfunctions. The anonymous function gets placed right into the middle of\r\nthe pipeline without a custom method or any other fancy workaround, and\r\nit just builds upon what R offers.\r\nOne drawback is that your code feels ‘old’ pretty fast on the\r\ntidyverse ecosystem. In this example alone, the |> operator called a\r\npipe, the new anonymous function syntax, the across, and the where\r\nfunctions from the package all have less than two years.\r\nThe tidyverse can also turn this code written in r, and with only a\r\nconnection to a data source, compile it into code for another language,\r\nit is mainly SQL code, but this is super helpful, as I will show later\r\non what I wished pandas implemented.\r\npandas\r\npandasI usually think of pandas as a project to copy into Python what\r\nworked on other languages, mainly what people call base R and some SQL\r\ninto Python, and it is hugely successful, and usage is most of the time\r\na joyful experience, it is not the prettiest, but it can get the job\r\ndone.\r\npandas is flexible enough to the point where you can write the same\r\ncode and make it feel like Base R or Tidyverse depending on what methods\r\nyou choose, for example, if you go heavy into indexing the base R\r\nstyle.\r\nWe can read its mission on the page\r\nMission\r\npandas aims to be the fundamental high-level building block for doing\r\npractical, real world data analysis in Python. Additionally, it has the\r\nbroader goal of becoming the most powerful and flexible open source data\r\nanalysis / manipulation tool available in any language.\r\nThat is quite a gredy statement, and I love it, pandas should strive\r\nfor perfection, for power and flexibility, but let’s try to see some\r\ncurrent limitations and quirks which I dislike\r\nSet up data\r\nWe can easily import the penguins dataset by reading the repository\r\ncsv.\r\n\r\npenguins = pd.read_csv('https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv')\r\n\r\nSlicing\r\nUsing loc, you get very close to base R philosophy.\r\n\r\ndf_result = penguins.loc[penguins.species.isin(['Adelie','Chinstrap']),['species','sex','body_mass_g']]\r\n\r\ndf_result\r\n       species     sex  body_mass_g\r\n0       Adelie    male       3750.0\r\n1       Adelie  female       3800.0\r\n2       Adelie  female       3250.0\r\n3       Adelie     NaN          NaN\r\n4       Adelie  female       3450.0\r\n..         ...     ...          ...\r\n339  Chinstrap    male       4000.0\r\n340  Chinstrap  female       3400.0\r\n341  Chinstrap    male       3775.0\r\n342  Chinstrap    male       4100.0\r\n343  Chinstrap  female       3775.0\r\n\r\n[220 rows x 3 columns]\r\n\r\nBut pandas gets ahead of itself and starts changing the data types\r\ndepending on the parameters, so what started out as a DataFrame may\r\nsometimes get back as a Series… You can avoid this behavior by passing\r\nlists on this example, this is similar to the data.table way, and it\r\nbaffles me why this is even a possibility, it overcharges the slicing\r\noperations into another capacity of object type manipulators, and that\r\nis common in Pandas and data.frame, slicing is this super powerful\r\nmethod that may return wildly different results depending on very little\r\nchange.\r\n\r\ndf_result = penguins.loc[penguins.species.isin(['Adelie','Chinstrap']),'species']\r\n\r\ndf_result\r\n0         Adelie\r\n1         Adelie\r\n2         Adelie\r\n3         Adelie\r\n4         Adelie\r\n         ...    \r\n339    Chinstrap\r\n340    Chinstrap\r\n341    Chinstrap\r\n342    Chinstrap\r\n343    Chinstrap\r\nName: species, Length: 220, dtype: object\r\n\r\nSimple Manipulation\r\nNumpy has a super similar syntax to the tidyverse if you opt into\r\nit\r\n\r\n\r\npenguins\\\r\n  .query(\"species in ('Adelie', 'Gentoo')\")\\\r\n  .groupby('species')\\\r\n  .agg({'body_mass_g':lambda x: np.mean(x)/1000})\r\n\r\n\r\n# penguins |>\r\n#   filter(species %in% c('Adelie', 'Gentoo'),\r\n#          sex == 'female') |> \r\n#   group_by(species) |> \r\n#   summarise(body_mass_g_to_kg = mean(body_mass_g)/1000)\r\n# df_full_indexes.columns = [\"_\".join(col_name).rstrip('_') for col_name in df_full_indexes.columns.to_flat_index()]\r\n# \r\n# df_full_indexes['B_sum']\r\n# \r\n# df_full_indexes['B_mean']\r\n         body_mass_g\r\nspecies             \r\nAdelie      3.700662\r\nGentoo      5.076016\r\n\r\nComplex Manipulations\r\npandas has this tendency to create more and more indexes, drop_index\r\nwill quickly become your go to solution, and when we add hierachical\r\nindexes to the mix, you are going to be copying and pasting some\r\nsolutions from Stack Overflow in order to flatten the data you created\r\nor you will need some sophisticated indexing operation to get some\r\nspecific results back.\r\nAnother details is that I need to manually drop the columns before\r\nthe groupby operation, and this sucks because there is no data type that\r\nwon’t exclude species (our grouping variable) while excluding island and\r\nsex\r\n\r\npenguins\\\r\n  .drop(columns = 'year')\\\r\n  .query(\"species in ('Adelie', 'Gentoo')\")\\\r\n  .groupby('species')\\\r\n  .select_dtypes('numeric')\r\nError in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'DataFrameGroupBy' object has no attribute 'select_dtypes'\r\n\r\nDetailed traceback:\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 904, in __getattr__\r\n    raise AttributeError(\r\n\r\n\r\nresult_hierarchical = penguins\\\r\n  .drop(columns = ['year','island','sex'])\\\r\n  .query(\"species in ('Adelie', 'Gentoo')\")\\\r\n  .groupby('species')\\\r\n  .agg([np.mean,np.median])\r\n\r\nresult_hierarchical\r\n        bill_length_mm        bill_depth_mm  ... flipper_length_mm  body_mass_g        \r\n                  mean median          mean  ...            median         mean  median\r\nspecies                                      ...                                       \r\nAdelie       38.791391   38.8     18.346358  ...             190.0  3700.662252  3700.0\r\nGentoo       47.504878   47.3     14.982114  ...             216.0  5076.016260  5000.0\r\n\r\n[2 rows x 8 columns]\r\n\r\nNow in order to access the body_mass_g columns and do a\r\ntransformation to kg I need to deal with the index system without using\r\nthe loc method.\r\n\r\nresult_hierarchical[\"body_mass_g\"] /1000\r\n             mean  median\r\nspecies                  \r\nAdelie   3.700662     3.7\r\nGentoo   5.076016     5.0\r\n\r\nSo you might just think, OK simple I just need to assign it back\r\n\r\nresult_hierarchical[\"body_mass_g\"] = result_hierarchical[\"body_mass_g\"] /1000\r\n\r\nand it works if you don’t mind losing the original data, if you want\r\nto create some new name\r\n\r\nresult_hierarchical[\"body_mass_g_back_to_g\"] = result_hierarchical[\"body_mass_g\"] * 1000\r\nError in py_call_impl(callable, dots$args, dots$keywords): ValueError: Expected a 1D array, got an array with shape (2, 2)\r\n\r\nDetailed traceback:\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\pandas\\core\\frame.py\", line 3645, in __setitem__\r\n    self._set_item_frame_value(key, value)\r\n  File \"C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\pandas\\core\\frame.py\", line 3788, in _set_item_frame_value\r\n    self._set_item_mgr(key, arraylike)\r\n  File \"C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\pandas\\core\\frame.py\", line 3802, in _set_item_mgr\r\n    self._mgr.insert(len(self._info_axis), key, value)\r\n  File \"C:\\Users\\bruno\\AppData\\Local\\R-MINI~1\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 1235, in insert\r\n    raise ValueError(\r\n\r\nInfuriating to say the least, you can go on a SO hunt to see the\r\nright approach to keep the indexes but at this point I am done with\r\npandas indexing and just cheat my way into the result with some\r\nflattened data frame\r\n\r\n# Flattern MultiIndex columns\r\nresult_hierarchical.columns = [\"_\".join(col_name).rstrip('_') for col_name in result_hierarchical.columns.to_flat_index()]\r\n\r\nresult_hierarchical['body_mass_g_median_back_to_g'] = result_hierarchical['body_mass_g_median'] * 1000\r\n\r\n\r\nresult_hierarchical['body_mass_g_median']\r\nspecies\r\nAdelie    3.7\r\nGentoo    5.0\r\nName: body_mass_g_median, dtype: float64\r\nresult_hierarchical['body_mass_g_median_back_to_g']\r\nspecies\r\nAdelie    3700.0\r\nGentoo    5000.0\r\nName: body_mass_g_median_back_to_g, dtype: float64\r\n\r\nPySpark/Koalas\r\nThis framework is delightful to work with especially because you can\r\ngo back and fourth between 4 apis SQL,Spark,Koalas and Pandas, and\r\nchances are one of them has a good approach to your problem, this post\r\nwould deviate too much if I talked in depth about this framework, but it\r\ncertainly has it’s place on the big data manipulation side, with some\r\napis that are sometimes superior to what pandas can offer, I will touch\r\nthe issue of laziness on the topic of what I wanted that Pandas\r\nimplemented, also PySpark really struggles with indexing as it should,\r\nand Koalas implements some crazy index rules.\r\nIt’s vital that you understand that any speed analysis among packages\r\non the individual personal computer level gets turned irrelevant as long\r\nas you get access to Spark Clusters, this is how you can query billions\r\nof records with ease, not by having slightly faster sorts on a single\r\nmachine level.\r\nGrivances with pandas\r\nI classify two kinds of problems on the pandas framework, the first\r\nand honestly the simplest to explain are things that it implements and I\r\nthink it shouldn’t a lot were copied from base R and that is why it was\r\nimportant that you understood the timeline of the packages at the\r\nbeginning of this post, and the second are the new features that dplyr\r\nput into the table in recent years, features that when pandas was being\r\ncreated didn’t exist and that I hope the pandas or other packages teams\r\nwill eventually be able to integrate into the Python ecosystem.\r\nWhat pandas has and it\r\nshouldn’t\r\nThere are many things that are strange to when I have to work in\r\nPython the first in that the syntax looks like someone from base r tried\r\nto port everthing to Python and unfortunally suceeded… this creates some\r\nstrange patterns like the whole iloc vs loc vs [[]] debate, they all\r\nsuck and I firmily believe that 2d manipulation of data was a mistake, a\r\nmistake that Python chose to copy.\r\nWhat pandas hasn’t and it\r\nshould\r\nLast updated on\r\n\r\n[1] \"2022-03-01 21:50:03 -03\"\r\n\r\nReferences\r\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer\r\nArchipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/](https://github.com/allisonhorst/palmerpenguins/\r\n\r\n\r\n\r\n",
    "preview": "https://rstudio.github.io/distill/images/javascript-d3-preview.png",
    "last_modified": "2022-03-01T21:50:13-03:00",
    "input_file": "pandaswishes.knit.md"
  },
  {
    "path": "posts/2022-02-27-testing/",
    "title": "testing",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Bruno Testaguzza Carlin",
        "url": "https://twosidesdata.netlify.app/"
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nInital Data\r\nTesting Sub\r\n\r\nHopefully it works\r\nTesting Diffs\r\n\r\nInital Data\r\n\r\n\r\nShow my code\r\n\r\ndata <- c(1:20)\r\n\r\n\r\n\r\n\r\n\r\nplot(data)\r\n\r\n\r\n\r\n\r\nTesting Sub\r\n\r\n\r\ndata <- c(1:50)\r\n\r\n\r\n\r\n\r\n\r\nplot(data)\r\n\r\n\r\n\r\n\r\nHopefully it works\r\n\r\n\r\ndata <- c(1:100)\r\n\r\n\r\n\r\n\r\n\r\nplot(data)\r\n\r\n\r\n\r\n\r\nTesting Diffs\r\n\r\n\r\ndata <- c(1:200)\r\n\r\n\r\n\r\n\r\n\r\nplot(data)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "https://rstudio.github.io/distill/images/javascript-d3-preview.png",
    "last_modified": "2022-03-01T21:55:50-03:00",
    "input_file": "testing.knit.md"
  },
  {
    "path": "posts/2022-02-27-welcome/",
    "title": "welcome",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Bruno Testaguzza Carlin",
        "url": "https://twosidesdata.netlify.app/"
      }
    ],
    "date": "2022-02-27",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing,\nnative to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n\n\n\n",
    "preview": "posts/2022-02-27-welcome/welcome_files/figure-html5/jjj-1.png",
    "last_modified": "2022-02-28T17:37:28+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
